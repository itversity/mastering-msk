{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the configurations, I have used while creating AWS MSK Connector for S3 as Sink to consume the data from Kafka Topic and write to AWS S3.\n",
    "\n",
    "```text\n",
    "connector.class=io.confluent.connect.s3.S3SinkConnector\n",
    "s3.region=us-east-1\n",
    "flush.size=30\n",
    "schema.compatibility=NONE\n",
    "tasks.max=1\n",
    "topics=retail_logs\n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat\n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage\n",
    "s3.bucket.name=airetail\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
