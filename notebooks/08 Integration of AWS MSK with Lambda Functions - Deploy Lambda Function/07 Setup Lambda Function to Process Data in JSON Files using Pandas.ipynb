{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the instructions to create AWS Lambda Function to transform data into Parquet file format. The data will also follow the foler hierarchy of year, month and then day of month. The folder structure can be interpreted as partitions by Glue Catalog or Spark.\n",
    "\n",
    "* Make sure to add a Lambda Layer with pandas, pyarrow, s3fs, fsspec, etc. We will use latest version of Python (Python 3.7) in Cloud shell.\n",
    "* Create a Lambda Function with blue print to understand how metadata is passed on triggering Lambda Function using s3 Event Notification.\n",
    "* Once we create Lambda Function, we will work on configuring Lambda Trigger based on s3 Event Notification.\n",
    "* We need to copy a file into the relevant location in s3 to validate if the Lambda Function is triggered or not.\n",
    "* Update the Lambda Function with required Pandas based processing logic. It will be integrated with the Metadata passed when Lambda Function is triggered when object is placed in s3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the Lambda Function which acts as blue print\n",
    "# We will use Python 3.7 as the version in Cloudshell is Python 3.7\n",
    "\n",
    "import urllib\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for rec in event['Records']:\n",
    "        bucket_name = rec['s3']['bucket']['name']\n",
    "        key = urllib.parse.unquote(rec['s3']['object']['key'])\n",
    "\n",
    "        s3_client = boto3.client('s3')\n",
    "        log_messages = s3_client. \\\n",
    "            get_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=key\n",
    "            )['Body']. \\\n",
    "            read(). \\\n",
    "            decode('utf-8'). \\\n",
    "            splitlines()\n",
    "        print(json.loads(log_messages[0]))\n",
    "        print(f'Number of messages in the file {key} is {len(log_messages)}')\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'statusMessage': f'Successfully processed!!!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-10 00:40:49      29233 retail_logs+1+0000011280.bin\n",
      "2022-08-10 00:40:49      28978 retail_logs+1+0000011400.bin\n",
      "2022-08-10 00:40:49      29048 retail_logs+1+0000011520.bin\n",
      "2022-08-10 00:40:49      29734 retail_logs+1+0000011640.bin\n",
      "2022-08-10 00:40:49      29439 retail_logs+1+0000011760.bin\n",
      "2022-08-10 00:40:49      29007 retail_logs+1+0000011880.bin\n",
      "2022-08-10 00:40:49      29025 retail_logs+1+0000012000.bin\n",
      "2022-08-10 00:40:49      29611 retail_logs+1+0000012120.bin\n",
      "2022-08-10 00:40:49      29323 retail_logs+1+0000012240.bin\n",
      "2022-08-10 00:40:49      30258 retail_logs+1+0000012360.bin\n",
      "2022-08-10 00:40:49      29398 retail_logs+1+0000012480.bin\n",
      "2022-08-10 00:40:49      28936 retail_logs+1+0000012600.bin\n",
      "2022-08-10 00:40:49      29523 retail_logs+1+0000012720.bin\n",
      "2022-08-10 00:40:49      29239 retail_logs+1+0000012840.bin\n",
      "2022-08-10 00:40:49      29321 retail_logs+1+0000012960.bin\n",
      "2022-08-10 00:40:49      29246 retail_logs+1+0000013080.bin\n",
      "2022-08-10 00:40:49      29286 retail_logs+1+0000013200.bin\n",
      "2022-08-10 00:40:49      29383 retail_logs+1+0000013320.bin\n",
      "2022-08-10 00:40:49      29298 retail_logs+1+0000013440.bin\n",
      "2022-08-10 00:40:49      28912 retail_logs+1+0000013560.bin\n",
      "2022-08-10 00:40:49      29332 retail_logs+1+0000013680.bin\n",
      "2022-08-10 00:40:49      29324 retail_logs+1+0000013800.bin\n",
      "2022-08-10 00:40:49      29127 retail_logs+1+0000013920.bin\n",
      "2022-08-10 00:40:49      29716 retail_logs+1+0000014040.bin\n",
      "2022-08-10 00:40:49      29270 retail_logs+1+0000014160.bin\n",
      "2022-08-10 00:40:49      29163 retail_logs+1+0000014280.bin\n",
      "2022-08-10 00:40:49      29115 retail_logs+1+0000014400.bin\n",
      "2022-08-10 00:40:49      29076 retail_logs+1+0000014520.bin\n",
      "2022-08-10 00:40:49      29141 retail_logs+1+0000014640.bin\n",
      "2022-08-10 00:40:49      29673 retail_logs+1+0000014760.bin\n",
      "2022-08-10 00:40:49      29711 retail_logs+1+0000014880.bin\n",
      "2022-08-10 00:40:49      29306 retail_logs+1+0000015000.bin\n",
      "2022-08-10 00:40:49      29163 retail_logs+1+0000015120.bin\n",
      "2022-08-10 00:40:49      29116 retail_logs+1+0000015240.bin\n",
      "2022-08-10 00:40:49      28956 retail_logs+1+0000015360.bin\n",
      "2022-08-10 00:40:49      29338 retail_logs+1+0000015480.bin\n",
      "2022-08-10 00:40:49      29331 retail_logs+1+0000015600.bin\n",
      "2022-08-10 00:40:49      29425 retail_logs+1+0000015720.bin\n",
      "2022-08-10 00:40:49      28907 retail_logs+1+0000015840.bin\n",
      "2022-08-10 00:40:49      28935 retail_logs+1+0000015960.bin\n",
      "2022-08-10 00:40:49      29104 retail_logs+1+0000016080.bin\n",
      "2022-08-10 00:40:49      29265 retail_logs+1+0000016200.bin\n",
      "2022-08-10 00:40:49      29096 retail_logs+1+0000016320.bin\n",
      "2022-08-10 00:40:49      29187 retail_logs+1+0000016440.bin\n",
      "2022-08-10 00:40:49      29332 retail_logs+1+0000016560.bin\n",
      "2022-08-10 00:40:49      29358 retail_logs+1+0000016680.bin\n",
      "2022-08-10 00:40:49      29221 retail_logs+1+0000016800.bin\n",
      "2022-08-10 00:40:49      29183 retail_logs+1+0000016920.bin\n",
      "2022-08-10 00:40:49      29492 retail_logs+1+0000017040.bin\n",
      "2022-08-10 00:40:49      28991 retail_logs+1+0000017160.bin\n",
      "2022-08-10 00:40:49      28988 retail_logs+1+0000017280.bin\n",
      "2022-08-10 00:40:49      29561 retail_logs+1+0000017400.bin\n",
      "2022-08-10 00:40:49      29284 retail_logs+1+0000017520.bin\n",
      "2022-08-10 00:40:49      29324 retail_logs+1+0000017640.bin\n",
      "2022-08-10 00:40:49      28941 retail_logs+1+0000017760.bin\n",
      "2022-08-10 00:40:49      29138 retail_logs+1+0000017880.bin\n",
      "2022-08-10 00:40:49      29209 retail_logs+1+0000018000.bin\n",
      "2022-08-10 00:40:49      29382 retail_logs+1+0000018120.bin\n",
      "2022-08-10 00:40:49      28890 retail_logs+1+0000018240.bin\n",
      "2022-08-10 00:40:49      29482 retail_logs+1+0000018360.bin\n",
      "2022-08-10 00:40:49      29489 retail_logs+1+0000018480.bin\n",
      "2022-08-10 00:40:49      29288 retail_logs+1+0000018600.bin\n",
      "2022-08-10 00:40:50      29088 retail_logs+1+0000018720.bin\n",
      "2022-08-10 00:40:50      29029 retail_logs+1+0000018840.bin\n",
      "2022-08-10 00:40:50      29566 retail_logs+1+0000018960.bin\n",
      "2022-08-10 00:40:50      29114 retail_logs+1+0000019080.bin\n",
      "2022-08-10 00:40:50      29327 retail_logs+1+0000019200.bin\n",
      "2022-08-10 00:40:50      29242 retail_logs+1+0000019320.bin\n",
      "2022-08-10 00:40:50      29282 retail_logs+1+0000019440.bin\n",
      "2022-08-10 00:40:50      29156 retail_logs+1+0000019560.bin\n",
      "2022-08-10 00:40:50      29478 retail_logs+1+0000019680.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://airetail/topics/retail_logs_backup/partition=1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_body = s3_client.get_object(\n",
    "    Bucket='airetail',\n",
    "    Key='topics/retail_logs_backup/partition=1/retail_logs+1+0000011280.bin'\n",
    ")['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'B12Q4JS450M1DQXN',\n",
       "  'HostId': 'VXFStNt+I0yMOortPibcTxEYFCtt8AXOGQ+umiolyuOpn4Yyo6wfKnHBU5Ul3Zoj6Q4YOs9lef0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'VXFStNt+I0yMOortPibcTxEYFCtt8AXOGQ+umiolyuOpn4Yyo6wfKnHBU5Ul3Zoj6Q4YOs9lef0=',\n",
       "   'x-amz-request-id': 'B12Q4JS450M1DQXN',\n",
       "   'date': 'Thu, 11 Aug 2022 01:14:08 GMT',\n",
       "   'etag': '\"9cc1854dd974d3e0e51be5c8f17f1f83\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"9cc1854dd974d3e0e51be5c8f17f1f83\"'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.put_object(\n",
    "    Body=object_body,\n",
    "    Bucket='airetail',\n",
    "    Key='topics/retail_logs/partition=1/retail_logs+1+0000011280.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 01:14:08      29233 retail_logs+1+0000011280.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://airetail/topics/retail_logs/partition=1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print('Processing Started')\n",
    "    for rec in event['Records']:\n",
    "        bucket_name = rec['s3']['bucket']['name']\n",
    "        object_key = urllib.parse.unquote(rec['s3']['object']['key'])\n",
    "\n",
    "        df = pd.read_json(\n",
    "            f's3://{bucket_name}/{object_key}',\n",
    "            lines=True\n",
    "        )\n",
    "\n",
    "        df['year'] = df['date'].apply(lambda dt: str(dt)[:4])\n",
    "        df['month'] = df['date'].apply(lambda dt: str(dt)[5:7])\n",
    "        df['dayofmonth'] = df['date'].apply(lambda dt: str(dt)[8:10])\n",
    "        df_grouped = df.groupby(['year', 'month', 'dayofmonth'])\n",
    "        orig_file_name = object_key.split('/')[-1].split('.')[0]\n",
    "        for key in df_grouped.groups.keys():\n",
    "            df_by_key = df_grouped.get_group(key).drop(labels=['year', 'month', 'dayofmonth'], axis=1)\n",
    "            year = list(key)[0]\n",
    "            month = list(key)[1]\n",
    "            dayofmonth = list(key)[2]\n",
    "            file_name = f'{orig_file_name}.snappy.parquet'\n",
    "            df_by_key.to_parquet(f's3://{bucket_name}/silver/retail_logs/year={year}/month={month}/dayofmonth={dayofmonth}/{file_name}')\n",
    "        print(f'{df.shape[0]} records in s3://{bucket_name}/{object_key} are converted to Parquet Format')\n",
    "    print('Processing is successfully done')\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'statusMessage': f'Successfully processed!!!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
